{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updates to my first Kaggle kernel in the hope to make it better.\n"
    ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, HuberRegressor, Ridge\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split,  cross_val_score\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import re\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(\"../input\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/train.csv')\n",
    "test = pd.read_csv('../input/test.csv')\n",
    "sample_submission = pd.read_csv('../input/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing purposes\n",
    "#train = train.iloc[:10000,:]\n",
    "#test = test.iloc[:10000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['comment_text'].fillna(\"_na_\", inplace=True)\n",
    "test['comment_text'].fillna(\"_na_\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, preprocessing the URLs to be uniform had minimal effect on my CV scores.\n",
    "# As does processing internal wikipedia references (though it saw a teeny-tiny improvement)\n",
    "# Adding this step did raise my competition result by .0029, which will matter more if I get my rank way up\n",
    "\n",
    "mod_comments =[]\n",
    "URLReg = re.compile(r'(http|https)://[^\\s]*')\n",
    "WikiReg = re.compile(r'(Wikipedia|Image):[^\\s]*') #finds all reference to internal wikipedia tags\n",
    "for comment in train['comment_text']:\n",
    "    comment = re.sub(URLReg, 'httpaddr', comment)\n",
    "    comment = re.sub(WikiReg, 'wikitag', comment)\n",
    "    mod_comments.append(comment)\n",
    "new_comments_df = pd.DataFrame({'comment_text': mod_comments})\n",
    "    \n",
    "train.update(new_comments_df)\n",
    "\n",
    "\n",
    "# Note that this kernel has less preprocessing than my other kernel. That might account for the lower error. \n",
    "# It was definitely an oversight from mismanaging how I coordinated testing.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train['comment_text']\n",
    "test_text = test['comment_text']\n",
    "all_data = train.iloc[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headings = list(train.columns.values)\n",
    "comment_headings = headings[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, I'll explore the data a little bit to get a sense of what I'm dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print(train['comment_text'][i] + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General impressions of the training data:\n",
    "\n",
    "    Length varies significantly.\n",
    "    On average the comments are fine.\n",
    "    My model should predict a comment is toxic if it also predicts it to be severe_toxic.\n",
    "\n",
    "Time to vectorize the data and start learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comment Processing\n",
    "\n",
    "Inspiration from this part comes from Bojan Tunguz's kernel: Logistic Regression with words and char n-grams.\n",
    "\n",
    "Vectorize the comments into word and char n-grams. The rational is that these can encode information differently. For example, users might obsfucate swear words .\n",
    "\n",
    "Bojan's justification for this approach: \"People often try to obfuscate bad words with additional characters. Using character n-grams can potentially detect those.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO play with settings of vectorizer further\n",
    "\n",
    "#all_text = pd.concat([train_text, test_text])\n",
    "\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    strip_accents='unicode', \n",
    "    stop_words='english',\n",
    "    lowercase=False, #because usage of all caps is likely indicate of naughty behavior\n",
    "    sublinear_tf=True,\n",
    "    ngram_range=(1,1),\n",
    "    max_features=10000)\n",
    "#word_vectorizer.fit(all_text)\n",
    "word_vectorizer.fit(train_text)\n",
    "train_text_word_transform = word_vectorizer.transform(train_text)\n",
    "test_text_word_transform = word_vectorizer.transform(test_text)\n",
    "\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "    analyzer='char', \n",
    "    strip_accents='unicode', \n",
    "    stop_words='english',\n",
    "    lowercase=False, #because usage of all caps is likely indicate of naughty behavior\n",
    "    sublinear_tf=True,\n",
    "    ngram_range=(2,6), #TODO I want to set the upper bound based off average word length, I think\n",
    "    max_features=50000)\n",
    "#char_vectorizer.fit(all_text)\n",
    "char_vectorizer.fit(train_text)\n",
    "train_text_char_transform = char_vectorizer.transform(train_text)\n",
    "test_text_char_transform = char_vectorizer.transform(test_text)\n",
    "\n",
    "complete_train_text = hstack((train_text_word_transform, train_text_char_transform))\n",
    "complete_test_text = hstack((test_text_word_transform, test_text_char_transform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_text_word_transform.shape)\n",
    "print(train_text_char_transform.shape)\n",
    "print(complete_train_text.shape)\n",
    "print(all_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "\n",
    "I'm going to use an ensemble model using a voting classifier. The models will be logistic regressison, multinomial Naive Bayes, and Random Forest with two different criterion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment/uncomment this if running for testing.\n",
    "\"\"\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(complete_train_text, all_data, test_size=0.3)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "#for vote in ['hard', 'soft']:\n",
    "#for est in [20,30, 40]:\n",
    "pred = {}\n",
    "cv_scores =[]\n",
    "for category in comment_headings:\n",
    "    clf1 = LogisticRegression(\n",
    "            C=1.0,\n",
    "            solver='sag',\n",
    "            max_iter=1000)\n",
    "    clf2 = MultinomialNB(\n",
    "        alpha=.3)\n",
    "    #clf3 = Ridge(alpha=1.0,solver='sag',max_iter=1000)\n",
    "    #clf4 = HuberRegressor(epislon=1.35, alpha=0.0001, max_iter=100)\n",
    "    clf3 = RandomForestClassifier(\n",
    "        n_estimators=est,\n",
    "        criterion='gini')\n",
    "    clf4 = RandomForestClassifier(\n",
    "        n_estimators=est,\n",
    "        criterion='entropy')\n",
    "\n",
    "    ensemble_clf = VotingClassifier(\n",
    "        estimators=[('lr', clf1), ('mNB', clf2), ('rf1',clf3), ('rf2', clf4)],\n",
    "        voting='hard')\n",
    "    cv_score = cross_val_score(ensemble_clf, complete_train_text, all_data[category], cv=5)\n",
    "    print(f'Cross-validation score for {category}: {cv_score}})\n",
    "    #cv_scores.append(cv_score)\n",
    "#print(f'Overall cross-validation score for {est}: {sum(cv_scores)/6}')\n",
    "    \n",
    "end = time.time()\n",
    "print(end-start)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment/uncomment this depending on if running for submission\n",
    "\n",
    "pred = {}\n",
    "cv_scores =[]\n",
    "for category in comment_headings:\n",
    "    clf1 = LogisticRegression(\n",
    "            C=1.0,\n",
    "            solver='sag',\n",
    "            max_iter=1000)\n",
    "    clf2 = MultinomialNB(\n",
    "        alpha=.3)\n",
    "    clf3 = RandomForestClassifier(\n",
    "        n_estimators=20,\n",
    "        criterion='gini')\n",
    "    clf4 = RandomForestClassifier(\n",
    "        n_estimators=20,\n",
    "        criterion='entropy')\n",
    "\n",
    "    ensemble_clf = VotingClassifier(\n",
    "        estimators=[('lr', clf1), ('mNB', clf2), ('rf1',clf3), ('rf2', clf4)],\n",
    "        voting='soft')\n",
    "    ensemble_clf.fit(complete_train_text, all_data[category])\n",
    "    cv_score = ensemble_clf.score(complete_train_text, all_data[category])\n",
    "    cv_scores.append(cv_score)\n",
    "    print(f'Score for {category} on entire training set: {cv_score}')\n",
    "    pred[category] = ensemble_clf.predict_proba(complete_test_text)\n",
    "    pred[category] = pred[category][:,1]\n",
    "print(f'Overall score on entire training set: {sum(cv_scores)/6}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_id = pd.DataFrame({'id': test[\"id\"]})\n",
    "submission = pd.concat([submission_id, pd.DataFrame(pred, columns = headings[2:])], axis=1)\n",
    "submission.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final result: 0.9772 where first place was 0.9885"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
