{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My first Kaggle kernel!\n",
    "\n",
    "I'm going to take my first stab at the Toxic Comments challenge using a simple SVM. In the course of doing this I've learned that I can't throw all the data into a single SVM (I forgot about scaling issues). I'm using a simple baggling classfier to split up the data into an ensemble so that I can mitigate the quadratic (IIRC) scaling of SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import re\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(\"../input\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/train.csv')\n",
    "test = pd.read_csv('../input/test.csv')\n",
    "sample_submission = pd.read_csv('../input/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing purposes\n",
    "#train = train.iloc[:10000,:]\n",
    "#test = test.iloc[:10000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['comment_text'].fillna(\"_na_\", inplace=True)\n",
    "test['comment_text'].fillna(\"_na_\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, preprocessing the URLs to be uniform had minimal effect on my CV scores.\n",
    "# As does processing internal wikipedia references (though it saw a teeny-tiny improvement)\n",
    "# Adding this step did raise my competition result by .0029, which will matter more if I get my rank way up\n",
    "\n",
    "mod_comments =[]\n",
    "URLReg = re.compile(r'(http|https)://[^\\s]*')\n",
    "WikiReg = re.compile(r'(Wikipedia|Image|Help):[^\\s]*') #finds all reference to internal wikipedia tags\n",
    "IPReg = re.compile(r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b')\n",
    "NumReg = re.compile(r'[0-9]+')\n",
    "HTMLReg = re.compile(r'\\|[\\w\\s\\\"\\#-\\:\\;\\!\\?\\%\\=]+=[\\w\\s\\\"\\#-\\:\\;\\!\\?\\%\\=\\@\\^\\&]+') #I think this gets some stuff it shouldn't.... but it also gets a lot of the junk html looking code.\n",
    "for comment in train['comment_text']:\n",
    "    comment = re.sub(URLReg, 'httpaddr', comment)\n",
    "    comment = re.sub(WikiReg, 'wikitag', comment)\n",
    "    comment = re.sub(IPReg, 'IPaddress', comment)\n",
    "    comment = re.sub(NumReg, 'number', comment)\n",
    "    comment = re.sub(HTMLReg, 'htmlcode', comment)\n",
    "    mod_comments.append(comment)\n",
    "new_comments_df = pd.DataFrame({'comment_text': mod_comments})\n",
    "    \n",
    "train.update(new_comments_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_text = train['comment_text']\n",
    "test_text = test['comment_text']\n",
    "all_data = train.iloc[:,2:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "headings = list(train.columns.values)\n",
    "comment_headings = headings[2:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, I'll explore the data a little bit to get a sense of what I'm dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print(train['comment_text'][i] + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General impressions of the training data:\n",
    "\n",
    "    Length varies significantly.\n",
    "    On average the comments are fine.\n",
    "    My model should predict a comment is toxic if it also predicts it to be severe_toxic.\n",
    "\n",
    "Time to vectorize the data and start learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comment Processing\n",
    "\n",
    "Inspiration from this part comes from Bojan Tunguz's kernel: Logistic Regression with words and char n-grams.\n",
    "\n",
    "Vectorize the comments into word and char n-grams. The rational is that these can encode information differently. For example, users might obsfucate swear words .\n",
    "\n",
    "Bojan's justification for this approach: \"People often try to obfuscate bad words with additional characters. Using character n-grams can potentially detect those.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO play with settings of vectorizer further\n",
    "\n",
    "#all_text = pd.concat([train_text, test_text])\n",
    "\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    strip_accents='unicode', \n",
    "    stop_words='english',\n",
    "    lowercase=False, #because usage of all caps is likely indicate of naughty behavior\n",
    "    sublinear_tf=True,\n",
    "    ngram_range=(1,1),\n",
    "    max_features=10000)\n",
    "#word_vectorizer.fit(all_text)\n",
    "word_vectorizer.fit(train_text)\n",
    "train_text_word_transform = word_vectorizer.transform(train_text)\n",
    "test_text_word_transform = word_vectorizer.transform(test_text)\n",
    "\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "    analyzer='char', \n",
    "    strip_accents='unicode', \n",
    "    stop_words='english',\n",
    "    lowercase=False, #because usage of all caps is likely indicate of naughty behavior\n",
    "    sublinear_tf=True,\n",
    "    ngram_range=(2,6), #TODO I want to set the upper bound based off average word length, I think\n",
    "    max_features=50000)\n",
    "#char_vectorizer.fit(all_text)\n",
    "char_vectorizer.fit(train_text)\n",
    "train_text_char_transform = char_vectorizer.transform(train_text)\n",
    "test_text_char_transform = char_vectorizer.transform(test_text)\n",
    "\n",
    "complete_train_text = hstack((train_text_word_transform, train_text_char_transform))\n",
    "complete_test_text = hstack((test_text_word_transform, test_text_char_transform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_text_word_transform.shape)\n",
    "print(train_text_char_transform.shape)\n",
    "print(complete_train_text.shape)\n",
    "print(all_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Simple logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment/uncomment this if running for testing.\n",
    "\"\"\"\n",
    "#X_train, X_test, y_train, y_test = train_test_split(complete_train_text, all_data, test_size=0.3)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "pred = {}\n",
    "cv_scores =[]\n",
    "for category in comment_headings:\n",
    "    clf = LogisticRegression(\n",
    "            C=1.,\n",
    "            solver='sag',\n",
    "            max_iter=1000)\n",
    "    scores = cross_val_score(clf, complete_train_text_tSVD, all_data[category], cv=5)\n",
    "    print(f'CV scores for {category}: {scores}, and average: {sum(scores)/5}')\n",
    "        \n",
    "end = time.time()\n",
    "print(end-start)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment/uncomment this depending on if running for submission\n",
    "\n",
    "pred = {}\n",
    "cv_scores = []\n",
    "for category in comment_headings:\n",
    "    clf = LogisticRegression(\n",
    "            C=1.0,\n",
    "            solver='sag',\n",
    "            max_iter=1000)\n",
    "    clf.fit(complete_train_text, all_data[category])\n",
    "    cv_score = clf.score(complete_train_text, all_data[category])\n",
    "    cv_scores.append(cv_score)\n",
    "    print(f'Validation score for {category} on entire training set: {cv_score}')\n",
    "    pred[category] = clf.predict_proba(complete_test_text)\n",
    "    pred[category] = pred[category][:,1]\n",
    "print(f'Overall validation score: {sum(cv_scores)/6}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_id = pd.DataFrame({'id': test[\"id\"]})\n",
    "submission = pd.concat([submission_id, pd.DataFrame(pred, columns = headings[2:])], axis=1)\n",
    "submission.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final result: 0.9773 where first place was 0.9885"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
